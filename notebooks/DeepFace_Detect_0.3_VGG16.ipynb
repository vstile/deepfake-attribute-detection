{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-21T10:53:22.768167Z",
          "iopub.status.busy": "2024-11-21T10:53:22.767682Z",
          "iopub.status.idle": "2024-11-21T10:53:22.774197Z",
          "shell.execute_reply": "2024-11-21T10:53:22.773091Z",
          "shell.execute_reply.started": "2024-11-21T10:53:22.768114Z"
        },
        "id": "UtFBYT0AXth4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#/* *************************************************************************** */\n",
        "#/*                                                                             */\n",
        "#/*   XXXVIII PhD Course on Big Data & AI @ Mercatorum University               */\n",
        "#/*                                                                             */\n",
        "#/*   By: vstile <vittoriostile@gmail.com>                                      */\n",
        "#/*   Link-in-bio page: https://linktr.ee/vstile                                */\n",
        "#/*                                                                             */\n",
        "#/*   Created: 2024/11/21 by vstile                                             */\n",
        "#/*   Updated: 2025/03/24 by vstile                                             */\n",
        "#/*                                                                             */\n",
        "#/* *************************************************************************** */"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JthfTOXV16li",
        "outputId": "984afd10-60a1-474e-f4e1-e0d38a4575e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAyIpNr1Xth5"
      },
      "source": [
        "This implementation handles REAL videos and FAKE videos across five manipulation methods. The model uses VGG16 for feature extraction and a custom classifier for binary classification (REAL vs. FAKE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DO92rV4Xth5"
      },
      "source": [
        "1. Loading the dataset from the specified paths.\n",
        "2. Preprocessing frames to extract faces.\n",
        "3. Training the model using VGG16.\n",
        "4. Calculating and displaying the accuracy of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5cCa6ezXth5"
      },
      "source": [
        "# 1. Define Paths and Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-21T10:53:22.775821Z",
          "iopub.status.busy": "2024-11-21T10:53:22.775472Z",
          "iopub.status.idle": "2024-11-21T10:53:22.792834Z",
          "shell.execute_reply": "2024-11-21T10:53:22.791573Z",
          "shell.execute_reply.started": "2024-11-21T10:53:22.775786Z"
        },
        "id": "MNacjJvkXth5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # Paths to REAL and FAKE videos\n",
        "# REAL_VIDEOS_PATH = \"/Volumes/Macintosh SSD/Developer/DeepFace_Detect/original_sequences/youtube/c40/videos\"\n",
        "# FAKE_VIDEOS_PATHS = [\n",
        "#     \"/Volumes/Macintosh SSD/Developer/DeepFace_Detect/manipulated_sequences/Deepfakes/c40/videos\",\n",
        "#     \"/Volumes/Macintosh SSD/Developer/DeepFace_Detect/manipulated_sequences/Face2Face/c40/videos\",\n",
        "#     \"/Volumes/Macintosh SSD/Developer/DeepFace_Detect/manipulated_sequences/FaceShifter/c40/videos\",\n",
        "#     \"/Volumes/Macintosh SSD/Developer/DeepFace_Detect/manipulated_sequences/FaceSwap/c40/videos\",\n",
        "#     \"/Volumes/Macintosh SSD/Developer/DeepFace_Detect/manipulated_sequences/NeuralTextures/c40/videos\",\n",
        "# ]\n",
        "\n",
        "# LABELS = {\"REAL\": 0, \"FAKE\": 1}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96Pv3pXq1vCh"
      },
      "source": [
        "La stringa nella lista **REAL_VIDEOS_PATH** punta alla directory che contiene video REAL non manipolati.\n",
        "\n",
        "Ogni stringa nella lista **FAKE_VIDEOS_PATHS** rappresenta il percorso di una cartella specifica per una delle 5 tecniche di manipolazione (Deepfakes, Face2Face, ecc.) che contengono ognuna video FAKE manipolati:\n",
        "* **Deepfakes**: Manipolazione basata su GAN (reti generative avversarie o generative adversarial network).\n",
        "* **Face2Face**: Tecnica di manipolazione che sostituisce espressioni facciali in tempo reale.\n",
        "* **FaceShifter**: Tecnica avanzata di scambio di volti.\n",
        "* **FaceSwap**: Manipolazione che sostituisce l'intero volto con quello di un altro individuo.\n",
        "* **NeuralTextures**: Tecnica che manipola i dettagli della texture del volto.\n",
        "\n",
        "**REAL_VIDEOS_PATHS** e **FAKE_VIDEOS_PATHS** saranno passati come input alla funzione **load_dataset** o **load_subset_dataset** (definita più avanti)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZL0IvBcXth6"
      },
      "source": [
        "# 2. Functions for Data Extraction and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-21T10:53:22.796957Z",
          "iopub.status.busy": "2024-11-21T10:53:22.79592Z"
        },
        "id": "8bN7wlxLXth6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayzzrBzB1vCh"
      },
      "source": [
        "Riepilogo del Flusso:\n",
        "1. Apre il video e legge i frame in un ciclo.\n",
        "2. Salta i frame in base a frame_skip per ottimizzare la velocità.\n",
        "3. Ridimensiona e converte in scala di grigi i frame selezionati.\n",
        "4. Utilizza Haar Cascade per rilevare i volti.\n",
        "5. Estrae, ridimensiona e memorizza i volti insieme alle etichettee.\n",
        "6. Restituisce i volti estratti e le etichette."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DhyCbco01vCi"
      },
      "outputs": [],
      "source": [
        "# # Function to extract frames and detect faces from a SUBSET\n",
        "# def extract_faces_from_video(video_path, label, cascade_classifier, frame_skip=30): #Qui ho messo 30 per velocizzare il calcolo, di solito si usa 10\n",
        "#     faces = [] # Inizializzo lista vuota per momorizzare i frame\n",
        "#     labels = [] # Inizializzo lista vuota per le etichette REAL e FAKE\n",
        "#     video_origins = []  # Nuova lista per tracciare i video di origine\n",
        "#     cap = cv2.VideoCapture(video_path) # Oggetto VideoCapture di OpenCV, utilizzato per aprire e leggere il video specificato in video_path\n",
        "#     frame_count = 0 # Contatore per tracciare il numero di frame elaborati\n",
        "\n",
        "# # Ciclo per Elaborare il Video\n",
        "#     while cap.isOpened(): # cap.isOpened(): Verifica se il video è aperto e leggibile\n",
        "#         ret, frame = cap.read() # cap.read(): Legge un frame dal video\n",
        "#         if not ret: # ret: Valore booleano che indica se il frame è stato letto con successo\n",
        "#             break # Se invece il frame non può essere letto (es. fine del video), il ciclo viene interrotto\n",
        "\n",
        "# # Rilevamento dei volti con Haar Cascades\n",
        "#         if frame_count % frame_skip == 0: # Elabora un frame ogni frame_skip (es. ogni 10 frame, se frame_skip=10)\n",
        "#             frame = cv2.resize(frame, (640, 480))  # Ridimensiona il frame per velocizzare\n",
        "#             gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # Converte il frame in scala di grigi per facilitare il classificatori Haar Cascade\n",
        "#             faces_detected = cascade_classifier.detectMultiScale(\n",
        "#                 gray, scaleFactor=1.3, minNeighbors=5\n",
        "#             )\n",
        "\n",
        "#            # Elaborazione dei Volti Rilevati\n",
        "#             for (x, y, w, h) in faces_detected: # Per ogni volto rilevato\n",
        "#                 face = frame[y:y+h, x:x+w] # Viene estratta la regione del volto dal frame originale usando le coordinate rilevate\n",
        "#                 face_resized = cv2.resize(face, (224, 224))  # Ridimensiona a 224x224\n",
        "#                 faces.append(face_resized) # Aggiunge il volto estratto e ridimensionato alla lista faces\n",
        "#                 labels.append(label) # Aggiunge l'etichetta (REAL o FAKE) corrispondente al volto estratto\n",
        "#                 video_origins.append(video_path)  # Salva il percorso del video di origine\n",
        "\n",
        "#         frame_count += 1 # Incrementa il contatore dei frame (basato su frame_skip)\n",
        "\n",
        "#     cap.release() # Rilascia le risorse associate all'oggetto VideoCapture dopo che il video è stato completamente elaborato\n",
        "#     return np.array(faces), np.array(labels), video_origins # In output la lista dei volti estratti e lista delle etichette associate e i percorsi dei video\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W_XHmpAg1vCi"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Function to load the dataset\n",
        "# def load_subset_dataset(real_path, fake_paths, cascade_path, max_videos=10): # Qui aggiungo parametro max_videos per ridurre il numero di video analizzati per ogni directory\n",
        "#     face_cascade = cv2.CascadeClassifier(cascade_path)  # Carica il classificatore Haar\n",
        "\n",
        "#     faces = []\n",
        "#     labels = []\n",
        "#     origins = []  # Lista per tracciare le origini dei frame\n",
        "\n",
        "\n",
        "#     # Carica un sottoinsieme di video REAL\n",
        "#     for video in os.listdir(real_path)[:max_videos]:\n",
        "#         video_path = os.path.join(real_path, video)\n",
        "#         face_frames, face_labels, video_origins = extract_faces_from_video(\n",
        "#             video_path, LABELS[\"REAL\"], face_cascade\n",
        "#         ) # Ogni volto estratto dai video in queste directory viene associato all'etichetta LABELS[\"REAL\"], ovvero 0\n",
        "#         faces.extend(face_frames)\n",
        "#         labels.extend(face_labels)\n",
        "#         origins.extend(video_origins)\n",
        "\n",
        "#     # Carica un sottoinsieme di video FAKE\n",
        "#     for fake_path in fake_paths: #la lista fake_paths (che punta a FAKE_VIDEOS_PATHS) viene iterata per ogni sottocartella (Deepfakes/c40/videos; Face2Face/c40/videos etc...)\n",
        "#         for video in os.listdir(fake_path)[:max_videos]:\n",
        "#             video_path = os.path.join(fake_path, video)\n",
        "#             face_frames, face_labels, video_origins = extract_faces_from_video(\n",
        "#                 video_path, LABELS[\"FAKE\"], face_cascade\n",
        "#             ) # Ogni volto estratto dai video in queste directory viene associato all'etichetta LABELS[\"FAKE\"], ovvero 1\n",
        "#             faces.extend(face_frames)\n",
        "#             labels.extend(face_labels)\n",
        "#             origins.extend(video_origins)\n",
        "\n",
        "#     return np.array(faces), np.array(labels), origins #array faces con le immagini dei volti e array labels con le etichette numeriche corrispondenti (0 per REAL, 1 per FAKE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r7kYSyhF1vCi"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Load the dataset\n",
        "\n",
        "# # Definizione del percorso del file per il classificatore Haar Cascade\n",
        "# cascade_path = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
        "\n",
        "# # Esegui la funzione per iterare su tutte le directory e assocuare etichette REAL e FAKE con il percorso del file Haar Cascade\n",
        "# faces, labels, origins = load_subset_dataset(REAL_VIDEOS_PATH, FAKE_VIDEOS_PATHS, cascade_path)\n",
        "\n",
        "# # REAL_VIDEOS_PATH: Percorso per i video REAL.\n",
        "# # FAKE_VIDEOS_PATHS: Lista di percorsi alle directory dei video FAKE.\n",
        "# # cascade_path: File Haar Cascade per il rilevamento dei volti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZwLQ8Fl1vCi"
      },
      "source": [
        "Rilevamento dei volti con **Haar Cascades**\n",
        "\n",
        "Per il rilevamento dei volti usiamo Haar Cascades (principalmente perché non usa GPU)\n",
        "\n",
        "**cascade_classifier.detectMultiScale** Rileva i volti nel frame in scala di grigi utilizzando Haar Cascades.\n",
        "\n",
        "*Parametri:*\n",
        "* scaleFactor=1.3: Indica di quanto viene ridotta la dimensione del frame ad ogni scala per il rilevamento.\n",
        "* minNeighbors=5: Numero minimo di rettangoli vicini richiesti per considerare una regione come volto.\n",
        "\n",
        "*Output:*\n",
        "* Un array di coordinate (x, y, w, h) per ciascun volto rilevato."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vEnie-bXth6"
      },
      "source": [
        "# 3. Split Dataset into Training, Validation, and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0UYMLne1vCj"
      },
      "source": [
        "Riepilogo del Flusso:\n",
        "1. Normalizzazione: Convertiamo i valori RGB in un intervallo tra 0 e 1 per stabilità e uniformità.\n",
        "2. Divisione Set: I dati vengono suddivisi in training, validation e test per addestrare, monitorare e valutare il modello.\n",
        "3. Stratificazione: Si mantiene la proporzione delle classi (REAL/FAKE) uniforme tra i diversi set, evitando squilibri."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Uu3zz6VvXth6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # Normalize RGB values\n",
        "# faces = faces / 255.0\n",
        "\n",
        "# # Split dataset 1\n",
        "# X_train, X_temp, y_train, y_temp, origins_train, origins_temp  = train_test_split(faces, labels, origins, test_size=0.28, stratify=labels, random_state=42)\n",
        "\n",
        "# # Split dataset 2\n",
        "# X_val, X_test, y_val, y_test, origins_val, origins_test = train_test_split(X_temp, y_temp, origins_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "# print(f\"Training set size: {len(X_train)}\")\n",
        "# print(f\"Validation set size: {len(X_val)}\")\n",
        "# print(f\"Test set size: {len(X_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m8dE9bW1vCj",
        "outputId": "90b11700-3374-4f1e-c7d3-a0500b864175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inizio caricamento facce dai video FAKE\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Percorsi delle directory contenenti le facce estratte dai video FAKE e REAL\n",
        "FAKE_FACES_PATH = \"/content/drive/MyDrive/Faces_skip_20/FAKE_faces\"\n",
        "REAL_FACES_PATH = \"/content/drive/MyDrive/Faces_skip_20/REAL_faces\"\n",
        "\n",
        "# Etichette per i video REAL e FAKE\n",
        "LABELS = {\"REAL\": 0, \"FAKE\": 1}\n",
        "\n",
        "# Inizializza le liste per le facce, le etichette e le origini\n",
        "faces = []\n",
        "labels = []\n",
        "origins = []\n",
        "\n",
        "# Funzione per caricare le immagini da una directory e assegnare le etichette\n",
        "def load_faces_from_directory(directory, label, origin_label):\n",
        "    count = 0\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".jpeg\"):\n",
        "            img_path = os.path.join(directory, filename)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                faces.append(img)\n",
        "                labels.append(label)\n",
        "                origins.append(origin_label)\n",
        "                count += 1\n",
        "    print(f\"Caricate {count} immagini dalla directory {directory}\")\n",
        "\n",
        "# Carica le facce dai video FAKE\n",
        "print(\"Inizio caricamento facce dai video FAKE\")\n",
        "load_faces_from_directory(FAKE_FACES_PATH, LABELS[\"FAKE\"], \"FAKE\")\n",
        "\n",
        "# Carica le facce dai video REAL\n",
        "print(\"Inizio caricamento facce dai video REAL\")\n",
        "load_faces_from_directory(REAL_FACES_PATH, LABELS[\"REAL\"], \"REAL\")\n",
        "\n",
        "# Converti le liste in array numpy\n",
        "print(\"Converti delle liste in array numpy\")\n",
        "faces = np.array(faces)\n",
        "labels = np.array(labels)\n",
        "origins = np.array(origins)\n",
        "print(\"Conversione effettuata delle liste in array numpy\")\n",
        "\n",
        "# Normalize RGB values\n",
        "print(\"Inizio normalizzazione di faces\")\n",
        "faces = faces / 255.0\n",
        "print(\"Normalizzazione effettuata correttamente per faces\")\n",
        "\n",
        "# Stampa il numero totale di facce caricate\n",
        "print(f\"Totale facce caricate: {len(faces)}\")\n",
        "print(f\"Totale etichette caricate: {len(labels)}\")\n",
        "print(f\"Totale origini caricate: {len(origins)}\")\n",
        "\n",
        "# Split del dataset in training, validation e test set\n",
        "print(\"Inizio split del dataset\")\n",
        "X_train, X_temp, y_train, y_temp, origins_train, origins_temp = train_test_split(faces, labels, origins, test_size=0.28, random_state=42)\n",
        "X_val, X_test, y_val, y_test, origins_val, origins_test = train_test_split(X_temp, y_temp, origins_temp, test_size=0.5, random_state=42)\n",
        "print(\"Split del dataset completato\")\n",
        "\n",
        "# Stampa delle dimensioni dei dataset dopo lo split\n",
        "print(f\"Training set: {len(X_train)} samples\")\n",
        "print(f\"Validation set: {len(X_val)} samples\")\n",
        "print(f\"Test set: {len(X_test)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95IN32PK1vCk"
      },
      "source": [
        "**Normalize RGB values**\n",
        "Converte i valori dei pixel da un intervallo [0, 255] a un intervallo [0, 1].\n",
        "I pixel nelle immagini digitali RGB hanno valori compresi tra 0 (nero) e 255 (bianco). Dividendo tutti i valori per 255, otteniamo una scala normalizzata tra 0 e 1. Per varie ragioni nei modelli di deep learning funzionano meglio con valori normalizzati.\n",
        "\n",
        "**Split dataset 1**\n",
        "\n",
        "**train_test_split**: Una funzione di scikit-learn utilizzata per dividere il dataset in modo casuale.\n",
        "\n",
        "*Parametri:*\n",
        "* faces: Dati delle immagini (features).\n",
        "* labels: Etichette associate (REAL o FAKE).\n",
        "* test_size=0.28: Il 28% dei dati totali viene assegnato al set temporaneo X_temp.\n",
        "* stratify=labels: Assicura che la proporzione di etichette (REAL/FAKE) sia mantenuta costante tra training e il resto.\n",
        "* random_state=42: Seed per la riproducibilità dei risultati.\n",
        "\n",
        "*Output:*\n",
        "* X_train: Immagini per il training.\n",
        "* y_train: Etichette per il training.\n",
        "* X_temp: Immagini temporanee per essere ulteriormente divise.\n",
        "* y_temp: Etichette temporanee.\n",
        "\n",
        "**Split dataset 2**\n",
        "\n",
        "Questa divisione prende il set temporaneo (28%) e lo divide equamente in:\n",
        "1. Validation Set 14%: Che serve per monitorare le prestazioni del modello durante il training. Utilizzato per determinare quando il modello inizia a sovradattarsi (overfitting).\n",
        "2. Test Set 14%: Che serve per valutare le prestazioni finali del modello dopo l'addestramento. Non viene utilizzato durante il training per evitare bias.\n",
        "\n",
        "*Parametri:*\n",
        "* X_temp e y_temp: Immagini ed etichette temporanee.\n",
        "* test_size=0.5: Divide il set temporaneo equamente tra validation e test.\n",
        "* stratify=y_temp: Mantiene la distribuzione di etichette (REAL/FAKE) in output dalla precedente.\n",
        "* random_state=42: Stesso seed utilizzato per la riproducibilità dei risultati."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTafgnQO1vCk"
      },
      "source": [
        "**Risultati Attesi**\n",
        "Supponendo che ci siano 859 immagini totali:\n",
        "\n",
        "* Training Set: 72% → 618 immagini.\n",
        "* Validation Set: 14% → 120 immagini.\n",
        "* Test Set: 14% → 121 immagini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAZCmptQXth6"
      },
      "source": [
        "# 4. Build and Compile VGG16 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywyp9ODZ1vCk"
      },
      "source": [
        "Riepilogo del Flusso:\n",
        "1. Caricare il Modello VGG16 Pre-Addestrato\n",
        "2. Congelare i Livelli del Modello di Base\n",
        "3. Aggiungere Livelli di Classificazione Personalizzati\n",
        "4. Creare il Modello Completo\n",
        "5. Compilare il Modello\n",
        "6. Mostrare in output un Sommario del Modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbZzaFQfXth6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Carica il modello pre-addestrato VGG16 senza il top\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Congela i pesi del modello pre-addestrato\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Aggiungi i tuoi livelli di classificazione\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Crea il modello\n",
        "model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "# Compila il modello\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Addestra il modello\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val), batch_size=32)\n",
        "\n",
        "# Valuta il modello\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2nczOLX1vCl"
      },
      "source": [
        "**Step 4: Creare il Modello Completo**\n",
        "\n",
        "**Funzione: model**:\n",
        "* inputs=base_model.input: L'input del modello è lo stesso della ResNet50 pre-addestrata (immagini 224x224x3).\n",
        "* outputs=output: L'output finale è il livello di classificazione binaria personalizzato.\n",
        "\n",
        "**Step 5: Compilare il Modello**\n",
        "\n",
        "**Ottimizzatore: optimizer='adam'**\n",
        "* Utilizza l'algoritmo di ottimizzazione Adam, che combina i vantaggi di RMSprop e Stochastic Gradient Descent (SGD).\n",
        "* Efficiente per problemi complessi e richiede poca regolazione manuale.\n",
        "\n",
        "**Funzione di Perdita: loss='binary_crossentropy'**\n",
        "* Adatta ai problemi di classificazione binaria.\n",
        "* Misura la differenza tra le probabilità previste dal modello (valori sigmoidi tra 0 e 1) e le etichette binarie reali (0 o 1).\n",
        "\n",
        "**Metriche di Valutazione: metrics=['accuracy']**\n",
        "* Valuta il modello durante il training e la validazione basandosi sull'accuratezza.\n",
        "\n",
        "**Step 6: Mostrare un Sommario del Modello model.summary()**\n",
        "\n",
        "Stampa una descrizione dettagliata del modello:\n",
        "* Architettura.\n",
        "* Numero di parametri (trainabili e non trainabili).\n",
        "* Livelli aggiunti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDtzE3WcXth7"
      },
      "source": [
        "# 5. Visualize Training Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQKGMmAG1vCm"
      },
      "source": [
        "Riepilogo del Flusso:\n",
        "1. Tracciare l'accuratezza in % (accuracy)\n",
        "2. Tracciare la perdita (loss)\n",
        "\n",
        "Questi valori sono memorizzati nell'oggetto history, che è il risultato di model.fit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGojOJ75Xth7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy') # history.history['accuracy']: Contiene i valori di accuratezza calcolati dal modello su ogni epoca per il training set\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy') # history.history['val_accuracy']: valori calcolati sul validation set dopo ogni epoca, ci dice quanto bene il modello generalizza su dati non visti durante il training\n",
        "plt.legend() # Stampa la leggenda in basso a dx\n",
        "plt.title('Accuracy') # Da il titolo al grafico in alto\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Train Loss') # history.history['loss']: i valori della funzione di perdita calcolati dal modello sul training set durante ogni epoca\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss') # history.history['val_loss']: i valori calcolati sul validation set dopo ogni epoca, ci dice quanto bene il modello generalizza su dati non visti\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUmNy2uH1vCm"
      },
      "source": [
        "# 6. Identify FAKE videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10epnhlx1vCm"
      },
      "source": [
        "Per identificare i video di origine dei frame dichiarati FAKE in fase di testing, è stata tracciata l'associazione tra ciascun frame e il video da cui è stato estratto tramite un dizionario che memorizza l'origine dei frame, così da poter analizzare i risultati dei test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b85b97Bs1vCm"
      },
      "outputs": [],
      "source": [
        "# Test del modello\n",
        "predictions = model.predict(X_test)  # Probabilità per ogni frame\n",
        "predicted_labels = (predictions > 0.79).astype(int)  # Soglia: 0.79\n",
        "\n",
        "# Trova i frame predetti come FAKE\n",
        "fake_indices = np.where(predicted_labels == 1)[0]  # Indici dei frame FAKE\n",
        "\n",
        "# Identifica i video di origine per i frame FAKE\n",
        "fake_videos = [origins_test[i] for i in fake_indices]\n",
        "\n",
        "print(f\"Numero di frame FAKE identificati: {len(fake_indices)}\")\n",
        "print(\"Video di origine per i frame FAKE:\")\n",
        "for video in set(fake_videos):  # Usa set() per evitare duplicati\n",
        "    print(video)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "history_visible": true,
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6134918,
          "sourceId": 9971791,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30786,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}